{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2591828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\tobybarker\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\tobybarker\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdcb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataEngineerAccelerator\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e906bec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#read_csv_to_df(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product\").printSchema()\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m productcategory_df \u001b[38;5;241m=\u001b[39m productcategory_df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;241m*\u001b[39m\u001b[43mschema\u001b[49m\u001b[38;5;241m.\u001b[39mfieldNames())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'schema' is not defined"
     ]
    }
   ],
   "source": [
    "product_df = spark.read.csv(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product/date=20220606/Product.csv\", header=\"True\")\n",
    "productcategory_df = spark.read.csv(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/ProductCategory/date=20220606/ProductCategory.csv\", header=\"True\")\n",
    "sales_df = spark.read.csv(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Sales/*\", header=\"True\")\n",
    "\n",
    "def read_csv_to_df(file_path):\n",
    "    df = spark.read.csv(file_path, header=\"True\")\n",
    "    return df\n",
    "\n",
    "read_csv_to_df(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323649b",
   "metadata": {},
   "source": [
    "# Writing files to Cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ae6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        current_date|\n",
      "+--------------------+\n",
      "|2023-10-18 10:22:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_date_df = spark.range(1).select(to_timestamp(current_timestamp()).alias(\"current_date\"))\n",
    "current_date_df.show()\n",
    "\n",
    "year_df = current_date_df.select(year(\"current_date\").alias(\"submission_year\"))\n",
    "month_df = current_date_df.select(month(\"current_date\").alias(\"submission_month\"))\n",
    "day_df = current_date_df.select(day(\"current_date\").alias(\"submission_day\"))\n",
    "hour_df = current_date_df.select(hour(\"current_date\").alias(\"submission_hour\"))\n",
    "minute_df = current_date_df.select(minute(\"current_date\").alias(\"submission_minute\"))\n",
    "second_df = current_date_df.select(second(\"current_date\").alias(\"submission_second\"))\n",
    "\n",
    "submission_year = year_df.first()[\"submission_year\"]\n",
    "submission_month = month_df.first()[\"submission_month\"]\n",
    "submission_day = day_df.first()[\"submission_day\"]\n",
    "submission_hour = hour_df.first()[\"submission_hour\"]\n",
    "submission_minute = minute_df.first()[\"submission_minute\"]\n",
    "submission_second = second_df.first()[\"submission_second\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87dbae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales\n"
     ]
    }
   ],
   "source": [
    "def DataFeed(file_path):\n",
    "    df = spark.read.csv(file_path, header=\"True\")\n",
    "    df = df.withColumn(\"sourcefile\",input_file_name())\n",
    "    df = df.withColumn(\"sourcefile\", substring_index(\"sourcefile\",\"/\", -1))\n",
    "    df = df.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"\\\\.\")[0]).select(\"sourcefile\").distinct()\n",
    "    df = df.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"%\")[0]).select(\"sourcefile\").distinct()\n",
    "    df = df.first()[\"sourcefile\"]\n",
    "    return df\n",
    "\n",
    "data_feed = DataFeed(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Sales/date=20220607\")\n",
    "print(data_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0491b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sourcefile|\n",
      "+----------+\n",
      "|   Product|\n",
      "+----------+\n",
      "\n",
      "Product\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product\", header=\"True\")\n",
    "df = df.withColumn(\"sourcefile\",input_file_name())\n",
    "df = df.withColumn(\"sourcefile\", substring_index(\"sourcefile\",\"/\", -1))\n",
    "df = df.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"\\\\.\")[0]).select(\"sourcefile\").distinct()\n",
    "df = df.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"%\")[0]).select(\"sourcefile\").distinct()\n",
    "\n",
    "df.show()\n",
    "df = df.first()[\"sourcefile\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad54c8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed/DataFeed=Sales/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=18/SubmissionHour=10/SubmissionMinute=22/SubmissionSecond=26\n"
     ]
    }
   ],
   "source": [
    "cleansed_path = f\"Cleansed/DataFeed={data_feed}/schemaVersion=1/SubmissionYear={submission_year}/SubmissionMonth={submission_month}/SubmissionDay={submission_day}/SubmissionHour={submission_hour}/SubmissionMinute={submission_minute}/SubmissionSecond={submission_second}\"\n",
    "print(cleansed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b130d",
   "metadata": {},
   "source": [
    "# Column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ed0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "productcategory_mapping = {\n",
    "    \"ID\" : (\"ProductCategoryID\", \"string\"),\n",
    "    \"Level1\" : (\"ProductCategoryName\", \"string\"),\n",
    "    \"Level2\" : (\"ProductSubCategoryName\", \"string\")\n",
    "}\n",
    "\n",
    "product_mapping = {\n",
    "    \"ProductID\" : (\"SourceProductID\", \"string\"),\n",
    "    \"ModelName\" : (\"ProductModelName\", \"string\"),\n",
    "    \"ProductName\" : (\"ProductName\", \"string\"),\n",
    "    \"ProductColor\" : (\"ProductColor\", \"string\"),\n",
    "    \"Size\" : (\"ProductSize\", \"string\"),\n",
    "    \"ProductWeight\" : (\"ProductWeightKilograms\", \"double\"),\n",
    "    \"UniqueProductNumber\" : (\"ProductUID\", \"string\"),\n",
    "    \"ProductCategoryID\" : (\"ProductCategoryID\", \"string\"),\n",
    "    \"StandardCost\" : (\"ProductCost\", \"double\"),\n",
    "    \"ListPrice\" : (\"ProductListPrice\", \"double\"),\n",
    "    \"ProductLine\" : (\"ProductLine\", \"string\"),\n",
    "    \"Class\" : (\"ProductClass\", \"string\"),\n",
    "    \"Style\" : (\"ProductStyle\", \"string\"),\n",
    "    \"ProductMakeFlag\" : (\"ProductMakeFlag\", \"string\"),\n",
    "    \"ProductFinishedGoodsFlag\" : (\"IsFinishedGood\", \"boolean\"),\n",
    "    \"SellStartDate\" : (\"ProductSellingStartDate\", \"string\"),\n",
    "    \"SellEndDate\" : (\"ProductSellingEndDate\", \"string\"),\n",
    "    \"ProductID\" : (\"SourceProductID\", \"string\"),\n",
    "    \"DiscontinuedDate\" : (\"ProductDiscontinuedDate\", \"string\"),\n",
    "    \"ModifiedDate\" : (\"ModifiedDate\", \"string\")\n",
    "}\n",
    "\n",
    "sales_mapping = {\n",
    "    \"SalesOrderID\" : (\"SalesOrderID\", \"string\"),\n",
    "    \"SalesOrderDetailID\" : (\"SalesOrderLineID\", \"string\"),\n",
    "    \"SalesOrderNumber\" : (\"SalesOrderNumber\", \"string\"),\n",
    "    \"OrderDate\" : (\"SalesOrderDate\", \"string\"),\n",
    "    \"ProductID\" : (\"ProductID\", \"string\"),\n",
    "    \"SpecialOfferID\" : (\"SpecialOfferID\", \"string\"),\n",
    "    \"SpecialOfferName\" : (\"SpecialOfferName\", \"string\"),\n",
    "    \"SpecialOfferDiscountPct\" : (\"SpecialOfferDiscountPct\", \"string\"),\n",
    "    \"SpecialOfferType\" : (\"SpecialOfferType\", \"string\"),\n",
    "    \"SpecialOfferCategory\" : (\"SpecialOfferCategory\", \"string\"),\n",
    "    \"SpecialOfferStartDate\" : (\"SpecialOfferStartDate\", \"string\"),\n",
    "    \"SpecialOfferEndDate\" : (\"SpecialOfferEndDate\", \"string\"),\n",
    "    \"OrderQty\" : (\"TotalUnits\", \"int\"),\n",
    "    \"UnitPrice\" : (\"UnitPrice\", \"double\"),\n",
    "    \"UnitPriceDiscount\" : (\"UnitPriceDiscountPercentage\", \"double\"),\n",
    "    \"LineTotal\" : (\"TotalLineValue\", \"double\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0aa10c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o252.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21) (host.docker.internal executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m schema \u001b[38;5;241m=\u001b[39m cleansed_product_category_schema\n\u001b[0;32m     52\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(productcategory_df\u001b[38;5;241m.\u001b[39mrdd, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Rows: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o252.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21) (host.docker.internal executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, LongType)\n",
    "\n",
    "cleansed_product_category_schema = StructType([\n",
    "    StructField(\"ProductCategoryID\", StringType()),\n",
    "    StructField(\"ProductCategoryName\", StringType()),\n",
    "    StructField(\"ProductSubCategoryName\", StringType())\n",
    "])    \n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"SourceProductID\", StringType()),\n",
    "    StructField(\"ProductModelName\", StringType()),\n",
    "    StructField(\"ProductName\", StringType()),\n",
    "    StructField(\"ProductColor\", StringType()),\n",
    "    StructField(\"ProductSize\", StringType()),\n",
    "    StructField(\"ProductWeightKilograms\", DoubleType()),\n",
    "    StructField(\"ProductUID\", StringType()),\n",
    "    StructField(\"ProductCategoryID\", StringType()),\n",
    "    StructField(\"ProductCost\", DoubleType()),\n",
    "    StructField(\"ProductListPrice\", DoubleType()),\n",
    "    StructField(\"ProductLine\", StringType()),\n",
    "    StructField(\"ProductClass\", StringType()),\n",
    "    StructField(\"ProductStyle\", StringType()),\n",
    "    StructField(\"ProductMakeFlag\", StringType()),\n",
    "    StructField(\"IsFinishedGood\", BooleanType()),\n",
    "    StructField(\"ProductSellingStartDate\", StringType()),\n",
    "    StructField(\"ProductSellingEndDate\", StringType()),\n",
    "    StructField(\"ProductDiscontinuedDate\", StringType()),\n",
    "    StructField(\"ModifiedDate\", StringType())\n",
    "])    \n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SalesOrderID\", StringType()),\n",
    "    StructField(\"SalesOrderID\", StringType()),\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderDate\", StringType()),\n",
    "    StructField(\"ProductID\", StringType()),\n",
    "    StructField(\"SpecialOfferID\", StringType()),\n",
    "    StructField(\"SpecialOfferName\", StringType()),\n",
    "    StructField(\"SpecialOfferDiscountPct\", DoubleType()),\n",
    "    StructField(\"SpecialOfferType\", StringType()),\n",
    "    StructField(\"SpecialOfferCategory\", StringType()),\n",
    "    StructField(\"SpecialOfferStartDate\", StringType()),\n",
    "    StructField(\"SpecialOfferEndDate\", StringType()),\n",
    "    StructField(\"TotalUnits\", IntegerType()),\n",
    "    StructField(\"UnitPrice\", DoubleType()),\n",
    "    StructField(\"UnitPriceDiscountPercentage\", DoubleType()),\n",
    "    StructField(\"TotalLineValue\", DoubleType())\n",
    "])\n",
    "    \n",
    "    \n",
    "schema = cleansed_product_category_schema\n",
    "df = spark.createDataFrame(productcategory_df.rdd, schema=schema)\n",
    "print(\"Number of Rows: \", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3726fd2",
   "metadata": {},
   "source": [
    "# Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e376e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------------------+\n",
      "|ProductCategoryID|ProductCategoryName|ProductSubCategory|\n",
      "+-----------------+-------------------+------------------+\n",
      "|                1|              Bikes|    Mountain Bikes|\n",
      "|                2|              Bikes|        Road Bikes|\n",
      "|                3|              Bikes|     Touring Bikes|\n",
      "|                4|         Components|        Handlebars|\n",
      "|                5|         Components|   Bottom Brackets|\n",
      "|                6|         Components|            Brakes|\n",
      "|                7|         Components|            Chains|\n",
      "|                8|         Components|         Cranksets|\n",
      "|                9|         Components|       Derailleurs|\n",
      "|               10|         Components|             Forks|\n",
      "|               11|         Components|          Headsets|\n",
      "|               12|         Components|   Mountain Frames|\n",
      "|               13|         Components|            Pedals|\n",
      "|               14|         Components|       Road Frames|\n",
      "|               15|         Components|           Saddles|\n",
      "|               16|         Components|    Touring Frames|\n",
      "|               17|         Components|            Wheels|\n",
      "|               18|           Clothing|        Bib-shorts|\n",
      "|               19|           Clothing|              Caps|\n",
      "|               20|           Clothing|            Gloves|\n",
      "+-----------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleansed_productcategory_df = productcategory_df.select(\n",
    "    *[\n",
    "        col(source_col).alias(target_col).cast(target_data_type)\n",
    "        for source_col, (target_col, target_data_type) in productcategory_mapping.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "for source_col, (target_col, target_data_type) in productcategory_mapping.items():\n",
    "    if target_data_type == \"string\":\n",
    "        cleansed_productcategory_df = cleansed_productcategory_df.withColumn(target_col, trim(initcap(col(target_col))))\n",
    "        \n",
    "\n",
    "cleansed_productcategory_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d350f",
   "metadata": {},
   "source": [
    "# Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fc5643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|ProductCost|\n",
      "+-----------+\n",
      "|    1059.31|\n",
      "|    1059.31|\n",
      "|      13.09|\n",
      "|      13.09|\n",
      "|        3.4|\n",
      "|        3.4|\n",
      "|      13.09|\n",
      "|       6.92|\n",
      "|      38.49|\n",
      "|      38.49|\n",
      "|      38.49|\n",
      "|      38.49|\n",
      "|     868.63|\n",
      "|     868.63|\n",
      "|     868.63|\n",
      "|     868.63|\n",
      "|     868.63|\n",
      "|     204.63|\n",
      "|     204.63|\n",
      "|     204.63|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleansed_product_df = product_df.select(\n",
    "    *[\n",
    "        col(source_col).alias(target_col).cast(target_data_type)\n",
    "        for source_col, (target_col, target_data_type) in product_mapping.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "for source_col, (target_col, target_data_type) in product_mapping.items():\n",
    "    if target_data_type == \"string\":\n",
    "        cleansed_product_df = cleansed_product_df.withColumn(target_col, trim(initcap(col(target_col))))\n",
    "    if target_data_type == \"double\":\n",
    "        cleansed_product_df = cleansed_product_df.withColumn(target_col, round(col(target_col), 2))\n",
    "    cleansed_product_df = cleansed_product_df.fillna(\"None\")\n",
    "cleansed_product_df = cleansed_product_df.withColumn(\"ProductWeightKilograms\", when(col(\"ProductWeightKilograms\") == 0, \"None\").otherwise(col(\"ProductWeightKilograms\")))\n",
    "cleansed_product_df = cleansed_product_df.withColumn(\"ProductWeightGrams\", (col(\"ProductWeightKilograms\")*1000).cast(\"int\"))\n",
    "cleansed_product_df = cleansed_product_df.withColumn(\"ProductProfitAtListPrice\", (col(\"ProductListPrice\") - col(\"ProductCost\")).cast(\"double\"))\n",
    "cleansed_product_df = cleansed_product_df.withColumn(\"ProductMarginAtListPrice\", (col(\"ProductProfitAtListPrice\") / col(\"ProductCost\")).cast(\"double\"))\n",
    "cleansed_product_df = cleansed_product_df.withColumn(\"ProductProfitAtListPrice\", round(col(\"ProductProfitAtListPrice\"), 2))\n",
    "cleansed_product_df = cleansed_product_df.withColumn(\"ProductMarginAtListPrice\", round(col(\"ProductMarginAtListPrice\"), 2))\n",
    "\n",
    "cleansed_product_df.select('ProductCost').show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc06c2e",
   "metadata": {},
   "source": [
    "# Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a900d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+----------------+----------------+---------+--------------+----------------+-----------------------+----------------+--------------------+---------------------+-------------------+----------+---------+---------------------------+--------------+-----------------+----------------------+\n",
      "|SalesOrderID|SalesOrderLineID|SalesOrderNumber|  SalesOrderDate|ProductID|SpecialOfferID|SpecialOfferName|SpecialOfferDiscountPct|SpecialOfferType|SpecialOfferCategory|SpecialOfferStartDate|SpecialOfferEndDate|TotalUnits|UnitPrice|UnitPriceDiscountPercentage|TotalLineValue|UnitDiscountValue|UnitPriceAfterDiscount|\n",
      "+------------+----------------+----------------+----------------+---------+--------------+----------------+-----------------------+----------------+--------------------+---------------------+-------------------+----------+---------+---------------------------+--------------+-----------------+----------------------+\n",
      "|       45266|            5717|            5717|01/01/2012 00:00|      777|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2| 2024.994|                          0|      4049.988|              0.0|               2024.99|\n",
      "|       45266|            5718|            5718|01/01/2012 00:00|      774|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         3| 2039.994|                          0|      6119.982|              0.0|               2039.99|\n",
      "|       45266|            5719|            5719|01/01/2012 00:00|      775|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2| 2024.994|                          0|      4049.988|              0.0|               2024.99|\n",
      "|       45266|            5720|            5720|01/01/2012 00:00|      743|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 714.7043|                          0|      714.7043|              0.0|                 714.7|\n",
      "|       45266|            5721|            5721|01/01/2012 00:00|      778|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2| 2024.994|                          0|      4049.988|              0.0|               2024.99|\n",
      "|       45266|            5722|            5722|01/01/2012 00:00|      772|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2| 2039.994|                          0|      4079.988|              0.0|               2039.99|\n",
      "|       45266|            5723|            5723|01/01/2012 00:00|      748|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2| 722.5949|                          0|     1445.1898|              0.0|                722.59|\n",
      "|       45267|            5724|            5724|01/01/2012 00:00|      762|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45267|            5725|            5725|01/01/2012 00:00|      770|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45267|            5726|            5726|01/01/2012 00:00|      758|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         3|  874.794|                          0|      2624.382|              0.0|                874.79|\n",
      "|       45268|            5727|            5727|01/01/2012 00:00|      766|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45268|            5728|            5728|01/01/2012 00:00|      760|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45269|            5729|            5729|01/01/2012 00:00|      775|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 2024.994|                          0|      2024.994|              0.0|               2024.99|\n",
      "|       45270|            5730|            5730|01/01/2012 00:00|      766|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2| 419.4589|                          0|      838.9178|              0.0|                419.46|\n",
      "|       45270|            5731|            5731|01/01/2012 00:00|      712|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         2|   5.1865|                          0|        10.373|              0.0|                  5.19|\n",
      "|       45270|            5732|            5732|01/01/2012 00:00|      763|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45270|            5733|            5733|01/01/2012 00:00|      765|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45270|            5734|            5734|01/01/2012 00:00|      749|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         3| 2146.962|                          0|      6440.886|              0.0|               2146.96|\n",
      "|       45270|            5735|            5735|01/01/2012 00:00|      767|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "|       45270|            5736|            5736|01/01/2012 00:00|      764|             1|     No Discount|                      0|     No Discount|         No Discount|     01/05/2011 00:00|   30/11/2014 00:00|         1| 419.4589|                          0|      419.4589|              0.0|                419.46|\n",
      "+------------+----------------+----------------+----------------+---------+--------------+----------------+-----------------------+----------------+--------------------+---------------------+-------------------+----------+---------+---------------------------+--------------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleansed_sales_df = sales_df.select(\n",
    "    *[\n",
    "        col(source_col).alias(target_col).cast(target_data_type)\n",
    "        for source_col, (target_col, target_data_type) in sales_mapping.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "for source_col, (target_col, target_data_type) in sales_mapping.items():\n",
    "    if target_data_type == \"string\":\n",
    "        cleansed_sales_df = cleansed_sales_df.withColumn(target_col, trim(initcap(col(target_col))))\n",
    "    if target_data_type == \"double\":\n",
    "        cleansed_sales_df = cleansed_sales_df.withColumn(target_col, round(col(target_col), 2))\n",
    "cleansed_sales_df = cleansed_sales_df.withColumn(\"UnitDiscountValue\", (col(\"UnitPrice\") * col(\"UnitPriceDiscountPercentage\")).cast(\"double\"))\n",
    "cleansed_sales_df = cleansed_sales_df.withColumn(\"UnitPriceAfterDiscount\", (col(\"UnitPrice\") - col(\"UnitDiscountValue\")).cast(\"double\"))\n",
    "cleansed_sales_df = cleansed_sales_df.withColumn(\"UnitDiscountValue\", round(col(\"UnitDiscountValue\"), 2))\n",
    "cleansed_sales_df = cleansed_sales_df.withColumn(\"UnitPriceAfterDiscount\", round(col(\"UnitPriceAfterDiscount\"), 2))\n",
    "cleansed_sales_df = cleansed_sales_df.fillna(0, subset=[\"UnitPriceDiscountPercentage\"])\n",
    "\n",
    "cleansed_sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6840cb9",
   "metadata": {},
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9896dc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+------------+-----------+----------------------+----------+-----------------+-----------+----------------+-----------+------------+------------+---------------+--------------+-----------------------+---------------------+-----------------------+------------+--------+------------------+------------------------+------------------------+\n",
      "|SourceProductID|    ProductModelName|         ProductName|ProductColor|ProductSize|ProductWeightKilograms|ProductUID|ProductCategoryID|ProductCost|ProductListPrice|ProductLine|ProductClass|ProductStyle|ProductMakeFlag|IsFinishedGood|ProductSellingStartDate|ProductSellingEndDate|ProductDiscontinuedDate|ModifiedDate|    date|ProductWeightGrams|ProductProfitAtListPrice|ProductMarginAtListPrice|\n",
      "+---------------+--------------------+--------------------+------------+-----------+----------------------+----------+-----------------+-----------+----------------+-----------+------------+------------+---------------+--------------+-----------------------+---------------------+-----------------------+------------+--------+------------------+------------------------+------------------------+\n",
      "|            680|       HL Road Frame|HL Road Frame - B...|       Black|         58|                  2.24|FR-R92B-58|               14|    1059.31|          1431.5|          R|           H|           U|           TRUE|          true|    30-04-2008 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2240|                  372.19|                    0.35|\n",
      "|            706|       HL Road Frame|HL Road Frame - R...|         Red|         58|                  2.24|FR-R92R-58|               14|    1059.31|          1431.5|          R|           H|           U|           TRUE|          true|    30-04-2008 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2240|                  372.19|                    0.35|\n",
      "|            707|           Sport-100|Sport-100 Helmet,...|         Red|       NULL|                  NULL| HL-U509-R|               31|      13.09|           34.99|          S|        NULL|        NULL|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    21.9|                    1.67|\n",
      "|            708|           Sport-100|Sport-100 Helmet,...|       Black|       NULL|                  NULL|   HL-U509|               31|      13.09|           34.99|          S|        NULL|        NULL|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    21.9|                    1.67|\n",
      "|            709| Mountain Bike Socks|Mountain Bike Soc...|       White|          M|                  NULL| SO-B909-M|               23|        3.4|             9.5|          M|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|  29-05-2012 00:00:00|                   NULL|        NULL|20220606|              NULL|                     6.1|                    1.79|\n",
      "|            710| Mountain Bike Socks|Mountain Bike Soc...|       White|          L|                  NULL| SO-B909-L|               23|        3.4|             9.5|          M|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|  29-05-2012 00:00:00|                   NULL|        NULL|20220606|              NULL|                     6.1|                    1.79|\n",
      "|            711|           Sport-100|Sport-100 Helmet,...|        Blue|       NULL|                  NULL| HL-U509-B|               31|      13.09|           34.99|          S|        NULL|        NULL|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    21.9|                    1.67|\n",
      "|            712|         Cycling Cap|        AWC Logo Cap|       Multi|       NULL|                  NULL|   CA-1098|               19|       6.92|            8.99|          S|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    2.07|                     0.3|\n",
      "|            713|Long-Sleeve Logo ...|Long-Sleeve Logo ...|       Multi|          S|                  NULL| LJ-0192-S|               21|      38.49|           49.99|          S|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    11.5|                     0.3|\n",
      "|            714|Long-Sleeve Logo ...|Long-Sleeve Logo ...|       Multi|          M|                  NULL| LJ-0192-M|               21|      38.49|           49.99|          S|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    11.5|                     0.3|\n",
      "|            715|Long-Sleeve Logo ...|Long-Sleeve Logo ...|       Multi|          L|                  NULL| LJ-0192-L|               21|      38.49|           49.99|          S|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    11.5|                     0.3|\n",
      "|            716|Long-Sleeve Logo ...|Long-Sleeve Logo ...|       Multi|         XL|                  NULL| LJ-0192-X|               21|      38.49|           49.99|          S|        NULL|           U|          FALSE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              NULL|                    11.5|                     0.3|\n",
      "|            717|       HL Road Frame|HL Road Frame - R...|         Red|         62|                   2.3|FR-R92R-62|               14|     868.63|          1431.5|          R|           H|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2300|                  562.87|                    0.65|\n",
      "|            718|       HL Road Frame|HL Road Frame - R...|         Red|         44|                  2.12|FR-R92R-44|               14|     868.63|          1431.5|          R|           H|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2120|                  562.87|                    0.65|\n",
      "|            719|       HL Road Frame|HL Road Frame - R...|         Red|         48|                  2.16|FR-R92R-48|               14|     868.63|          1431.5|          R|           H|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2160|                  562.87|                    0.65|\n",
      "|            720|       HL Road Frame|HL Road Frame - R...|         Red|         52|                   2.2|FR-R92R-52|               14|     868.63|          1431.5|          R|           H|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2200|                  562.87|                    0.65|\n",
      "|            721|       HL Road Frame|HL Road Frame - R...|         Red|         56|                  2.24|FR-R92R-56|               14|     868.63|          1431.5|          R|           H|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2240|                  562.87|                    0.65|\n",
      "|            722|       LL Road Frame|LL Road Frame - B...|       Black|         58|                  2.46|FR-R38B-58|               14|     204.63|          337.22|          R|           L|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2460|                  132.59|                    0.65|\n",
      "|            723|       LL Road Frame|LL Road Frame - B...|       Black|         60|                  2.48|FR-R38B-60|               14|     204.63|          337.22|          R|           L|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2480|                  132.59|                    0.65|\n",
      "|            724|       LL Road Frame|LL Road Frame - B...|       Black|         62|                   2.5|FR-R38B-62|               14|     204.63|          337.22|          R|           L|           U|           TRUE|          true|    31-05-2011 00:00:00|                 NULL|                   NULL|        NULL|20220606|              2500|                  132.59|                    0.65|\n",
      "+---------------+--------------------+--------------------+------------+-----------+----------------------+----------+-----------------+-----------+----------------+-----------+------------+------------+---------------+--------------+-----------------------+---------------------+-----------------------+------------+--------+------------------+------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_csv_to_df(file_path, schema):\n",
    "    df = spark.read.csv(file_path, header=\"True\", schema=schema)\n",
    "    return df\n",
    "\n",
    "def productcategory_transformations(file_path, schema):\n",
    "    df = read_csv_to_df(file_path, schema)\n",
    "\n",
    "    string_columns = [col_name for col_name, col_type in df.dtypes if col_type == \"string\"]\n",
    "    for col_name in string_columns:\n",
    "        df = df.withColumn(col_name, regexp_replace(col_name, \"\\\\t\", \" \"))\n",
    "        df = df.withColumn(col_name, trim(initcap(col(col_name))))\n",
    "    return df\n",
    "\n",
    "\n",
    "def product_transformations(file_path, schema):\n",
    "    df = read_csv_to_df(file_path, schema)\n",
    "    \n",
    "    string_columns = [col_name for col_name, col_type in df.dtypes if col_type == \"string\"]\n",
    "    for col_name in string_columns:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "    double_columns = [col_name for col_name, col_type in df.dtypes if col_type == \"double\"]\n",
    "    for col_name in double_columns: \n",
    "        df = df.withColumn(col_name, round(col(col_name), 2)).fillna(\"None\")\n",
    "    df = df.withColumn(\"ProductWeightKilograms\", when(col(\"ProductWeightKilograms\") == 0, None).otherwise(col(\"ProductWeightKilograms\")))\n",
    "    df = df.withColumn(\"ProductWeightGrams\", (col(\"ProductWeightKilograms\")*1000).cast(\"int\"))\n",
    "    df = df.withColumn(\"ProductProfitAtListPrice\", (col(\"ProductListPrice\") - col(\"ProductCost\")).cast(\"double\"))\n",
    "    df = df.withColumn(\"ProductMarginAtListPrice\", (col(\"ProductProfitAtListPrice\") / col(\"ProductCost\")).cast(\"double\"))\n",
    "    df = df.withColumn(\"ProductProfitAtListPrice\", round(col(\"ProductProfitAtListPrice\"), 2))\n",
    "    df = df.withColumn(\"ProductMarginAtListPrice\", round(col(\"ProductMarginAtListPrice\"), 2))\n",
    "    for column in df.columns:\n",
    "        df = df.withColumn(column, when(col(column).isNull(), None).otherwise(col(column)))\n",
    "        df = df.withColumn(column, when(col(column) == lit(\"NULL\"), None).otherwise(col(column)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def sales_transformations(file_path, schema):\n",
    "    df = read_csv_to_df(file_path, schema)\n",
    "\n",
    "    string_columns = [col_name for col_name, col_type in df.dtypes if col_type == \"string\"]\n",
    "    for col_name in string_columns:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "    double_columns = [col_name for col_name, col_type in df.dtypes if col_type == \"double\"]\n",
    "    for col_name in double_columns: \n",
    "        df = df.withColumn(col_name, round(col(col_name), 2))\n",
    "    df = df.withColumn(\"UnitDiscountValue\", (col(\"UnitPrice\") * col(\"UnitPriceDiscountPercentage\")).cast(\"double\"))\n",
    "    df = df.withColumn(\"UnitPriceAfterDiscount\", (col(\"UnitPrice\") - col(\"UnitDiscountValue\")).cast(\"double\"))\n",
    "    df = df.withColumn(\"UnitDiscountValue\", round(col(\"UnitDiscountValue\"), 2))\n",
    "    df = df.withColumn(\"UnitPriceAfterDiscount\", round(col(\"UnitPriceAfterDiscount\"), 2))\n",
    "    df = df.fillna(0, subset=[\"UnitPriceDiscountPercentage\"])\n",
    "    return df\n",
    "\n",
    "def transformer(file_path, schema):\n",
    "    if \"ProductCategory\" in file_path:\n",
    "        df = productcategory_transformations(file_path, schema)\n",
    "    elif \"Product\" in file_path:\n",
    "        df = product_transformations(file_path, schema)\n",
    "    elif \"Sales\" in file_path:\n",
    "        df = sales_transformations(file_path, schema)\n",
    "    else:\n",
    "        return print(\"Unknown CSV file.\")\n",
    "    return df\n",
    "    \n",
    "\n",
    "def write_to_sink(file_path, schema):\n",
    "    current_date_df = spark.range(1).select(to_timestamp(current_timestamp()).alias(\"current_date\"))\n",
    "    year_df = current_date_df.select(year(\"current_date\").alias(\"submission_year\"))\n",
    "    month_df = current_date_df.select(month(\"current_date\").alias(\"submission_month\"))\n",
    "    day_df = current_date_df.select(day(\"current_date\").alias(\"submission_day\"))\n",
    "    hour_df = current_date_df.select(hour(\"current_date\").alias(\"submission_hour\"))\n",
    "    minute_df = current_date_df.select(minute(\"current_date\").alias(\"submission_minute\"))\n",
    "    second_df = current_date_df.select(second(\"current_date\").alias(\"submission_second\"))\n",
    "\n",
    "    submission_year = year_df.first()[\"submission_year\"]\n",
    "    submission_month = month_df.first()[\"submission_month\"]\n",
    "    submission_day = day_df.first()[\"submission_day\"]\n",
    "    submission_hour = hour_df.first()[\"submission_hour\"]\n",
    "    submission_minute = minute_df.first()[\"submission_minute\"]\n",
    "    submission_second = second_df.first()[\"submission_second\"]\n",
    "\n",
    "    df = transformer(file_path, schema)\n",
    "    data_feed = df.withColumn(\"sourcefile\",input_file_name())\n",
    "    data_feed = data_feed.withColumn(\"sourcefile\", substring_index(\"sourcefile\",\"/\", -1))\n",
    "    data_feed = data_feed.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"\\\\.\")[0]).select(\"sourcefile\").distinct()\n",
    "    data_feed = data_feed.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"%\")[0]).select(\"sourcefile\").distinct()\n",
    "    data_feed = data_feed.first()[\"sourcefile\"]\n",
    "\n",
    "    cleansed_path = f\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed={data_feed}/schemaVersion=1/SubmissionYear={submission_year}/SubmissionMonth={submission_month}/SubmissionDay={submission_day}/SubmissionHour={submission_hour}/SubmissionMinute={submission_minute}/SubmissionSecond={submission_second}\"\n",
    "    \n",
    "    df.write.mode(\"overwrite\").parquet(cleansed_path)\n",
    "    \n",
    "    \n",
    "product_transformations(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product\", product_schema).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21016832-bf62-4063-8b1d-524b8d45a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------+-----------------------+\n",
      "|ProductSellingStartDate|ProductSellingEndDate|ProductDiscontinuedDate|\n",
      "+-----------------------+---------------------+-----------------------+\n",
      "|    31-05-2011 00:00:00|                 Null|                   Null|\n",
      "|    31-05-2011 00:00:00|  29-05-2013 00:00:00|                   Null|\n",
      "|    31-05-2011 00:00:00|  29-05-2012 00:00:00|                   Null|\n",
      "|    30-05-2012 00:00:00|  29-05-2013 00:00:00|                   Null|\n",
      "|    30-05-2013 00:00:00|                 Null|                   Null|\n",
      "|    30-04-2008 00:00:00|                 Null|                   Null|\n",
      "|    30-05-2012 00:00:00|                 Null|                   Null|\n",
      "+-----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product = product_transformations(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product/date=20220606/Product.csv\")\n",
    "product.select(\"ProductSellingStartDate\", \"ProductSellingEndDate\", \"ProductDiscontinuedDate\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026db5a",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5306e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimProduct = StructType([\n",
    "    StructField(\"ProductKey\", LongType()),\n",
    "    StructField(\"ProductName\", StringType()),\n",
    "    StructField(\"ProductLineName\", StringType()),\n",
    "    StructField(\"ProductModelName\", StringType()),\n",
    "    StructField(\"ProductCategoryName\", StringType()),\n",
    "    StructField(\"ProductSubCategoryName\", StringType()),   \n",
    "    StructField(\"ProductColor\", StringType()),\n",
    "    StructField(\"ProductSize\", StringType()),\n",
    "    StructField(\"ProductWeightKilograms\", DoubleType()),\n",
    "    StructField(\"ProductCost\", DoubleType()),\n",
    "    StructField(\"ProductListPrice\", DoubleType()),\n",
    "    StructField(\"ProductProfitAtListPrice\", DoubleType()),\n",
    "    StructField(\"ProductMarginAtListPrice\", DoubleType()),\n",
    "    StructField(\"ProductLine\", StringType()),\n",
    "    StructField(\"ProductClass\", StringType()),\n",
    "    StructField(\"ProductStyle\", StringType()),\n",
    "    StructField(\"IsFinishedGood\", BooleanType()),\n",
    "    StructField(\"ProductSellingStartDate\", IntegerType()),\n",
    "    StructField(\"ProductSellingEndDate\", IntegerType()),\n",
    "    StructField(\"ProductDiscontinuedDate\", IntegerType()),\n",
    "    StructField(\"CurrencyKey\", StringType())\n",
    "])\n",
    "\n",
    "FactSales = StructType([\n",
    "    StructField(\"FactHashID\", LongType()),\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderDate\", IntegerType()),\n",
    "    StructField(\"ProductKey\", LongType()),\n",
    "    StructField(\"SpecialOfferKey\", LongType()),\n",
    "    StructField(\"UnitVolume\", IntegerType()),\n",
    "    StructField(\"CurrencyKey\", StringType()),\n",
    "    StructField(\"UnitPrice\", DoubleType()),\n",
    "    StructField(\"UnitPriceDiscountPercentage\", DoubleType()),\n",
    "    StructField(\"UnitPriceAfterDiscount\", DoubleType()),\n",
    "    StructField(\"TotalSalesLineValue\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f795eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesOrderDetailID</th>\n",
       "      <th>SalesOrderID</th>\n",
       "      <th>SalesOrderNumber</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>SpecialOfferID</th>\n",
       "      <th>SpecialOfferName</th>\n",
       "      <th>SpecialOfferDiscountPct</th>\n",
       "      <th>SpecialOfferType</th>\n",
       "      <th>SpecialOfferCategory</th>\n",
       "      <th>SpecialOfferStartDate</th>\n",
       "      <th>SpecialOfferEndDate</th>\n",
       "      <th>OrderQty</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>UnitPriceDiscount</th>\n",
       "      <th>LineTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5717</td>\n",
       "      <td>45266</td>\n",
       "      <td>SO45266</td>\n",
       "      <td>01/01/2012 00:00</td>\n",
       "      <td>777</td>\n",
       "      <td>1</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>0</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>01/05/2011 00:00</td>\n",
       "      <td>30/11/2014 00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2024.994</td>\n",
       "      <td>0</td>\n",
       "      <td>4049.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5718</td>\n",
       "      <td>45266</td>\n",
       "      <td>SO45266</td>\n",
       "      <td>01/01/2012 00:00</td>\n",
       "      <td>774</td>\n",
       "      <td>1</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>0</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>01/05/2011 00:00</td>\n",
       "      <td>30/11/2014 00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2039.994</td>\n",
       "      <td>0</td>\n",
       "      <td>6119.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5719</td>\n",
       "      <td>45266</td>\n",
       "      <td>SO45266</td>\n",
       "      <td>01/01/2012 00:00</td>\n",
       "      <td>775</td>\n",
       "      <td>1</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>0</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>01/05/2011 00:00</td>\n",
       "      <td>30/11/2014 00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2024.994</td>\n",
       "      <td>0</td>\n",
       "      <td>4049.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5720</td>\n",
       "      <td>45266</td>\n",
       "      <td>SO45266</td>\n",
       "      <td>01/01/2012 00:00</td>\n",
       "      <td>743</td>\n",
       "      <td>1</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>0</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>01/05/2011 00:00</td>\n",
       "      <td>30/11/2014 00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>714.7043</td>\n",
       "      <td>0</td>\n",
       "      <td>714.7043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5721</td>\n",
       "      <td>45266</td>\n",
       "      <td>SO45266</td>\n",
       "      <td>01/01/2012 00:00</td>\n",
       "      <td>778</td>\n",
       "      <td>1</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>0</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>No Discount</td>\n",
       "      <td>01/05/2011 00:00</td>\n",
       "      <td>30/11/2014 00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2024.994</td>\n",
       "      <td>0</td>\n",
       "      <td>4049.988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SalesOrderDetailID SalesOrderID SalesOrderNumber         OrderDate  \\\n",
       "0               5717        45266          SO45266  01/01/2012 00:00   \n",
       "1               5718        45266          SO45266  01/01/2012 00:00   \n",
       "2               5719        45266          SO45266  01/01/2012 00:00   \n",
       "3               5720        45266          SO45266  01/01/2012 00:00   \n",
       "4               5721        45266          SO45266  01/01/2012 00:00   \n",
       "\n",
       "  ProductID SpecialOfferID SpecialOfferName SpecialOfferDiscountPct  \\\n",
       "0       777              1      No Discount                       0   \n",
       "1       774              1      No Discount                       0   \n",
       "2       775              1      No Discount                       0   \n",
       "3       743              1      No Discount                       0   \n",
       "4       778              1      No Discount                       0   \n",
       "\n",
       "  SpecialOfferType SpecialOfferCategory SpecialOfferStartDate  \\\n",
       "0      No Discount          No Discount      01/05/2011 00:00   \n",
       "1      No Discount          No Discount      01/05/2011 00:00   \n",
       "2      No Discount          No Discount      01/05/2011 00:00   \n",
       "3      No Discount          No Discount      01/05/2011 00:00   \n",
       "4      No Discount          No Discount      01/05/2011 00:00   \n",
       "\n",
       "  SpecialOfferEndDate OrderQty UnitPrice UnitPriceDiscount LineTotal  \n",
       "0    30/11/2014 00:00        2  2024.994                 0  4049.988  \n",
       "1    30/11/2014 00:00        3  2039.994                 0  6119.982  \n",
       "2    30/11/2014 00:00        2  2024.994                 0  4049.988  \n",
       "3    30/11/2014 00:00        1  714.7043                 0  714.7043  \n",
       "4    30/11/2014 00:00        2  2024.994                 0  4049.988  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#product_df = product_transformations(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=Product/date=20220606/Product.csv\")\n",
    "#productcategory_df = productcategory_transformations(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/ProductCategory/date=20220606/ProductCategory.csv\")\n",
    "#sales_df = sales_transformations(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Sales\")\n",
    "\n",
    "product_df = spark.read.parquet(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=Product/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=30\")\n",
    "productcategory_df = spark.read.parquet(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=ProductCategory/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=30\")\n",
    "\n",
    "\n",
    "pandas_product = product_df.toPandas()\n",
    "pandas_sales = sales_df.toPandas()\n",
    "pandas_sales.head()\n",
    "#sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26508a29-0c17-4f2d-b89f-61c743d756c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductModelName: string (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- ProductColor: string (nullable = true)\n",
      " |-- ProductSize: string (nullable = true)\n",
      " |-- ProductWeightKilograms: double (nullable = true)\n",
      " |-- ProductCost: double (nullable = true)\n",
      " |-- ProductListPrice: double (nullable = true)\n",
      " |-- ProductLine: string (nullable = true)\n",
      " |-- ProductClass: string (nullable = true)\n",
      " |-- ProductStyle: string (nullable = true)\n",
      " |-- IsFinishedGood: boolean (nullable = true)\n",
      " |-- ProductSellingStartDate: integer (nullable = true)\n",
      " |-- ProductSellingEndDate: integer (nullable = true)\n",
      " |-- ProductDiscontinuedDate: integer (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- ProductProfitAtListPrice: double (nullable = true)\n",
      " |-- ProductMarginAtListPrice: double (nullable = true)\n",
      " |-- SubmissionHour: integer (nullable = true)\n",
      " |-- SubmissionMinute: integer (nullable = true)\n",
      " |-- SubmissionSecond: integer (nullable = true)\n",
      " |-- CurrencyKey: string (nullable = false)\n",
      " |-- ProductLineName: string (nullable = false)\n",
      " |-- ProductKey: long (nullable = false)\n",
      " |-- ProductCategoryName: string (nullable = true)\n",
      " |-- ProductSubCategoryName: string (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- SubmissionHour: integer (nullable = true)\n",
      " |-- SubmissionMinute: integer (nullable = true)\n",
      " |-- SubmissionSecond: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product = product_df\n",
    "date_columns = [\"ProductSellingStartDate\", \"ProductSellingEndDate\", \"ProductDiscontinuedDate\"]\n",
    "for column in date_columns:\n",
    "    product = product.withColumn(column, date_format(to_date(col(column), \"dd-MM-yyyy HH:mm:ss\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "\n",
    "product = product.withColumn(\"CurrencyKey\", lit(\"GBP\"))\n",
    "product = product.withColumn(\"ProductLineName\", when(col(\"ProductLine\") == \"M\", \"Mountain Bikes\")\n",
    "                                                .when(col(\"ProductLine\") == \"Null\", \"Bike Parts\")\n",
    "                                                .when(col(\"ProductLine\") == \"R\", \"Road Bikes\")\n",
    "                                                .when(col(\"ProductLine\") == \"S\", \"Accessories and Attire\")\n",
    "                                                .when(col(\"ProductLine\") == \"T\", \"Touring Bikes\")\n",
    "                                                .otherwise(\"Unknown\"))\n",
    "product = product.withColumn(\"ProductKey\", xxhash64(concat_ws(\"|\", col(\"SourceProductID\"), lit(\"Product\"), lit(\"ProductCategory\"))).cast(\"long\"))\n",
    "product = product.join(productcategory_df, on=\"ProductCategoryID\", how=\"left\")\n",
    "product = product.withColumnRenamed(\"ProductSubCategory\", \"ProductSubCategoryName\")\n",
    "for col_name in product.columns:\n",
    "    data_type = product.schema[col_name].dataType\n",
    "    if data_type == DoubleType:\n",
    "        product = product.withColumn(col_name, round(col(col_name), 2))\n",
    "product = product.drop(\"SourceProductID\", \"ProductUID\", \"ProductCategoryID\", \"ProductMakeFlag\", \"ModifiedDate\", \"ProductWeightGrams\")\n",
    "\n",
    "product.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c35c9f35-be2a-4d5e-9b82-f7d415864004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------+---------------------------+-------------------+----------------------+--------------------+-----------+--------------------+-------------------+\n",
      "|SalesOrderNumber|SalesOrderDate|UnitVolume|UnitPrice|UnitPriceDiscountPercentage|TotalSalesLineValue|UnitPriceAfterDiscount|          FactHashID|CurrencyKey|          ProductKey|    SpecialOfferKey|\n",
      "+----------------+--------------+----------+---------+---------------------------+-------------------+----------------------+--------------------+-----------+--------------------+-------------------+\n",
      "|         SO45266|      20120101|         2|  2024.99|                        0.0|            4049.99|               2024.99| -567102246171116547|        GBP|-7526531161666529242|7804943415359386941|\n",
      "|         SO45266|      20120101|         3|  2039.99|                        0.0|            6119.98|               2039.99| 2099134126650911946|        GBP|-5247391021351068957|7804943415359386941|\n",
      "|         SO45266|      20120101|         2|  2024.99|                        0.0|            4049.99|               2024.99|-7993808648578370316|        GBP|-8966359152163381396|7804943415359386941|\n",
      "|         SO45266|      20120101|         1|    714.7|                        0.0|              714.7|                 714.7| 1251657638759355600|        GBP| 1432488126338272493|7804943415359386941|\n",
      "|         SO45266|      20120101|         2|  2024.99|                        0.0|            4049.99|               2024.99|-4295931336226313672|        GBP| 5979043214446987507|7804943415359386941|\n",
      "|         SO45266|      20120101|         2|  2039.99|                        0.0|            4079.99|               2039.99|-8485537592250840898|        GBP|-6727538919361665971|7804943415359386941|\n",
      "|         SO45266|      20120101|         2|   722.59|                        0.0|            1445.19|                722.59| -542790974195279722|        GBP|-3049056916127618863|7804943415359386941|\n",
      "|         SO45267|      20120101|         1|   419.46|                        0.0|             419.46|                419.46| 4989389790702388835|        GBP| 6574018774085971166|7804943415359386941|\n",
      "|         SO45267|      20120101|         1|   419.46|                        0.0|             419.46|                419.46| 5217934923007662410|        GBP| 5203826774108117409|7804943415359386941|\n",
      "|         SO45267|      20120101|         3|   874.79|                        0.0|            2624.38|                874.79|-3726712055285665568|        GBP|-1203647811099998875|7804943415359386941|\n",
      "|         SO45268|      20120101|         1|   419.46|                        0.0|             419.46|                419.46| 2940079869939442288|        GBP| 7266755613305562492|7804943415359386941|\n",
      "|         SO45268|      20120101|         1|   419.46|                        0.0|             419.46|                419.46|-1059609226925311254|        GBP|-2009892855987404099|7804943415359386941|\n",
      "|         SO45269|      20120101|         1|  2024.99|                        0.0|            2024.99|               2024.99|-2786678791674986965|        GBP|-8966359152163381396|7804943415359386941|\n",
      "|         SO45270|      20120101|         2|   419.46|                        0.0|             838.92|                419.46| 5889457924369895124|        GBP| 7266755613305562492|7804943415359386941|\n",
      "|         SO45270|      20120101|         2|     5.19|                        0.0|              10.37|                  5.19| 5766415104502250367|        GBP|-2966093393230185734|7804943415359386941|\n",
      "|         SO45270|      20120101|         1|   419.46|                        0.0|             419.46|                419.46| 5408072008484298400|        GBP|-6020427406176896912|7804943415359386941|\n",
      "|         SO45270|      20120101|         1|   419.46|                        0.0|             419.46|                419.46| 7111003030793290009|        GBP|-4434430765868701265|7804943415359386941|\n",
      "|         SO45270|      20120101|         3|  2146.96|                        0.0|            6440.89|               2146.96| 8613783688667060465|        GBP|-6398976786578015852|7804943415359386941|\n",
      "|         SO45270|      20120101|         1|   419.46|                        0.0|             419.46|                419.46|-7042143798717401362|        GBP| 8133700843540540310|7804943415359386941|\n",
      "|         SO45270|      20120101|         1|   419.46|                        0.0|             419.46|                419.46|-2772774948764642051|        GBP|-7751725087721968504|7804943415359386941|\n",
      "+----------------+--------------+----------+---------+---------------------------+-------------------+----------------------+--------------------+-----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = sales_df\n",
    "date_columns = [\"SalesOrderDate\"]\n",
    "for column in date_columns:\n",
    "    sales = sales.withColumn(column, date_format(to_date(col(column), \"dd/MM/yyyy HH:mm\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    \n",
    "FactHashID_columns = sales.columns\n",
    "sales = sales.withColumn(\"FactHashID\", xxhash64(concat_ws(\"|\", *[col(column) for column in FactHashID_columns])).cast(\"long\"))\n",
    "sales = sales.withColumn(\"CurrencyKey\", lit(\"GBP\"))\n",
    "sales = sales.withColumn(\"ProductKey\", xxhash64(concat_ws(\"|\", col(\"ProductID\"), lit(\"Product\"), lit(\"ProductCategory\"))).cast(\"long\"))\n",
    "#FactHashID_columns = [\"SalesOrderNumber\", \"SalesOrderDate\", \"TotalUnits\", \"UnitPrice\", \"UnitPriceDiscountPercentage\", \"UnitPriceAfterDiscount\", \"TotalLineValue\"]\n",
    "#sales = sales.withColumn(\"FactHashID\", xxhash64(concat_ws(\"|\", *[col(column) for column in FactHashID_columns])).cast(\"long\"))\n",
    "sales = sales.withColumn(\"SpecialOfferKey\", xxhash64(concat_ws(\"|\", col(\"SpecialOfferID\"), lit(\"Sales\"))).cast(\"long\"))\n",
    "sales = sales.drop(\"SalesOrderID\", \"SalesOrderLineID\", \"ProductID\", \"SpecialOfferID\", \"SpecialOfferName\", \"SpecialOfferDiscountPct\", \"SpecialOfferType\", \"SpecialOfferCategory\", \"SpecialOfferStartDate\", \"SpecialOfferEndDate\", \"UnitDiscountValue\")\n",
    "sales = sales.withColumnRenamed(\"TotalUnits\", \"UnitVolume\").withColumnRenamed(\"TotalLineValue\", \"TotalSalesLineValue\")\n",
    "#Partition the file by Month and Year from the field SalesOrderDate\n",
    "\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "573d2136-0710-4e7e-b3c9-25a22af41341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Integrated/Domain=Master/Entity=Product/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=18/SubmissionHour=16/SubmissionMinute=34/SubmissionSecond=20\n"
     ]
    }
   ],
   "source": [
    "current_date_df = spark.range(1).select(to_timestamp(current_timestamp()).alias(\"current_date\"))\n",
    "year_df = current_date_df.select(year(\"current_date\").alias(\"submission_year\"))\n",
    "month_df = current_date_df.select(month(\"current_date\").alias(\"submission_month\"))\n",
    "day_df = current_date_df.select(day(\"current_date\").alias(\"submission_day\"))\n",
    "hour_df = current_date_df.select(hour(\"current_date\").alias(\"submission_hour\"))\n",
    "minute_df = current_date_df.select(minute(\"current_date\").alias(\"submission_minute\"))\n",
    "second_df = current_date_df.select(second(\"current_date\").alias(\"submission_second\"))\n",
    "\n",
    "submission_year = year_df.first()[\"submission_year\"]\n",
    "submission_month = month_df.first()[\"submission_month\"]\n",
    "submission_day = day_df.first()[\"submission_day\"]\n",
    "submission_hour = hour_df.first()[\"submission_hour\"]\n",
    "submission_minute = minute_df.first()[\"submission_minute\"]\n",
    "submission_second = second_df.first()[\"submission_second\"]\n",
    "\n",
    "df = spark.read.csv(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Product\", header=\"True\")\n",
    "entity = df.withColumn(\"sourcefile\",input_file_name())\n",
    "entity = entity.withColumn(\"sourcefile\", substring_index(\"sourcefile\",\"/\", -1))\n",
    "entity = entity.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"\\\\.\")[0]).select(\"sourcefile\").distinct()\n",
    "entity = entity.withColumn(\"sourcefile\", split(col(\"sourcefile\"), \"%\")[0]).select(\"sourcefile\").distinct()\n",
    "entity = entity.first()[\"sourcefile\"]\n",
    "if entity == \"Product\":\n",
    "    domain = \"Master\"\n",
    "elif entity == \"Sales\":\n",
    "    domain = \"Commercial\"\n",
    "\n",
    "integrated_path = f\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Integrated/Domain={domain}/Entity={entity}/schemaVersion=1/SubmissionYear={submission_year}/SubmissionMonth={submission_month}/SubmissionDay={submission_day}/SubmissionHour={submission_hour}/SubmissionMinute={submission_minute}/SubmissionSecond={submission_second}\"\n",
    "print(integrated_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c28f9dd1-6b20-47b8-8a92-08fc01e0f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrderID: string (nullable = true)\n",
      " |-- SalesOrderLineID: string (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- SalesOrderDate: string (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- SpecialOfferID: string (nullable = true)\n",
      " |-- SpecialOfferName: string (nullable = true)\n",
      " |-- SpecialOfferDiscountPct: string (nullable = true)\n",
      " |-- SpecialOfferType: string (nullable = true)\n",
      " |-- SpecialOfferCategory: string (nullable = true)\n",
      " |-- SpecialOfferStartDate: string (nullable = true)\n",
      " |-- SpecialOfferEndDate: string (nullable = true)\n",
      " |-- TotalUnits: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- UnitPriceDiscountPercentage: double (nullable = true)\n",
      " |-- TotalLineValue: double (nullable = true)\n",
      " |-- UnitDiscountValue: double (nullable = true)\n",
      " |-- UnitPriceAfterDiscount: double (nullable = true)\n",
      " |-- SubmissionMinute: integer (nullable = true)\n",
      " |-- SubmissionSecond: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_parquet_to_df(file_path):\n",
    "    df = spark.read.parquet(file_path, header=\"True\")\n",
    "    return df\n",
    "    \n",
    "def read_latest_parquet(file_path):\n",
    "    latest_file = None\n",
    "    latest_timestamp = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            file_timestamp = os.path.getctime(root)\n",
    "\n",
    "            if file_timestamp > latest_timestamp:\n",
    "                latest_timestamp = file_timestamp\n",
    "                latest_file = root\n",
    "            \n",
    "    if latest_file:\n",
    "        df = spark.read.parquet(latest_file)\n",
    "        return df\n",
    "\n",
    "\n",
    "def product_integrated(file_path):\n",
    "    df = read_parquet_to_df(file_path)\n",
    "    ProductCategory_df = read_latest_parquet(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=ProductCategory\")\n",
    "    DimProduct_df = df\n",
    "    date_columns = [\"ProductSellingStartDate\", \"ProductSellingEndDate\", \"ProductDiscontinuedDate\"]\n",
    "    for column in date_columns:\n",
    "        DimProduct_df = DimProduct_df.withColumn(column, date_format(to_date(col(column), \"dd-MM-yyyy HH:mm:ss\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "\n",
    "    DimProduct_df = DimProduct_df.withColumn(\"CurrencyKey\", lit(\"GBP\"))\n",
    "    DimProduct_df = DimProduct_df.withColumn(\"ProductLineName\", when(col(\"ProductLine\") == \"M\", \"Mountain Bikes\")\n",
    "                                                                .when(col(\"ProductLine\") == \"Null\", \"Bike Parts\")\n",
    "                                                                .when(col(\"ProductLine\") == \"R\", \"Road Bikes\")\n",
    "                                                                .when(col(\"ProductLine\") == \"S\", \"Accessories and Attire\")\n",
    "                                                                .when(col(\"ProductLine\") == \"T\", \"Touring Bikes\")\n",
    "                                                                .otherwise(\"Unknown\"))\n",
    "    DimProduct_df = DimProduct_df.withColumn(\"ProductKey\", xxhash64(concat_ws(\"|\", col(\"SourceProductID\"), lit(\"Product\"), lit(\"ProductCategory\"))).cast(\"long\"))\n",
    "    DimProduct_df = DimProduct_df.join(ProductCategory_df, on=\"ProductCategoryID\", how=\"left\")\n",
    "    DimProduct_df = DimProduct_df.withColumnRenamed(\"ProductSubCategory\", \"ProductSubCategoryName\")\n",
    "    for col_name in DimProduct_df.columns:\n",
    "        data_type = DimProduct_df.schema[col_name].dataType\n",
    "        DimProduct_df = DimProduct_df.withColumn(col_name, col(col_name).cast(data_type))\n",
    "        if data_type == DoubleType:\n",
    "            DimProduct_df = DimProduct_df.withColumn(col_name, round(col(col_name), 2))\n",
    "    DimProduct_df = DimProduct_df.select(DimProduct.fieldNames())\n",
    "    return DimProduct_df\n",
    "\n",
    "\n",
    "def sales_integrated(file_path):\n",
    "    df = read_parquet_to_df(file_path)\n",
    "\n",
    "    FactSales_df = df\n",
    "    date_columns = [\"SalesOrderDate\"]\n",
    "    for column in date_columns:\n",
    "        FactSales_df = FactSales_df.withColumn(column, date_format(to_date(to_timestamp(col(column), \"dd/MM/yyyy HH:mm\")), \"yyyyMMdd\").cast(\"int\"))\n",
    "\n",
    "    FactSales_df = FactSales_df.withColumn(\"CurrencyKey\", lit(\"GBP\"))\n",
    "    FactSales_df = FactSales_df.withColumn(\"ProductKey\", xxhash64(concat_ws(\"|\", col(\"ProductID\"), lit(\"Product\"), lit(\"ProductCategory\"))).cast(\"long\"))\n",
    "    FactHashID_columns = [\"SalesOrderNumber\", \"SalesOrderDate\", \"TotalUnits\", \"UnitPrice\", \"UnitPriceDiscountPercentage\", \"UnitPriceAfterDiscount\", \"TotalLineValue\"]\n",
    "    FactSales_df = FactSales_df.withColumn(\"FactHashID\", xxhash64(concat_ws(\"|\", *[col(column) for column in FactHashID_columns])).cast(\"long\"))\n",
    "    FactSales_df = FactSales_df.withColumn(\"SpecialOfferKey\", xxhash64(concat_ws(\"|\", col(\"SpecialOfferID\"), lit(\"Sales\"))).cast(\"long\"))\n",
    "    FactSales_df = FactSales_df.withColumnRenamed(\"TotalUnits\", \"UnitVolume\").withColumnRenamed(\"TotalLineValue\", \"TotalSalesLineValue\")\n",
    "    FactSales_df = FactSales_df.select(FactSales.fieldNames())\n",
    "    #Partition the file by Month and Year from the field SalesOrderDate\n",
    "    return df\n",
    "\n",
    "def transformer(file_path):\n",
    "    if \"Product\" in file_path:\n",
    "        df = product_integrated(file_path)\n",
    "    elif \"Sales\" in file_path:\n",
    "        df = sales_integrated(file_path)\n",
    "    else:\n",
    "        return print(\"Unknown parquet file.\")\n",
    "    return df\n",
    "\n",
    "def write_to_sink(file_path):\n",
    "    current_date_df = spark.range(1).select(to_timestamp(current_timestamp()).alias(\"current_date\"))\n",
    "    year_df = current_date_df.select(year(\"current_date\").alias(\"submission_year\"))\n",
    "    month_df = current_date_df.select(month(\"current_date\").alias(\"submission_month\"))\n",
    "    day_df = current_date_df.select(day(\"current_date\").alias(\"submission_day\"))\n",
    "    hour_df = current_date_df.select(hour(\"current_date\").alias(\"submission_hour\"))\n",
    "    minute_df = current_date_df.select(minute(\"current_date\").alias(\"submission_minute\"))\n",
    "    second_df = current_date_df.select(second(\"current_date\").alias(\"submission_second\"))\n",
    "\n",
    "    submission_year = year_df.first()[\"submission_year\"]\n",
    "    submission_month = month_df.first()[\"submission_month\"]\n",
    "    submission_day = day_df.first()[\"submission_day\"]\n",
    "    submission_hour = hour_df.first()[\"submission_hour\"]\n",
    "    submission_minute = minute_df.first()[\"submission_minute\"]\n",
    "    submission_second = second_df.first()[\"submission_second\"]\n",
    "\n",
    "    df = transformer(file_path)\n",
    "    if \"Product\" in file_path:\n",
    "        entity = \"Product\"\n",
    "        domain = \"Master\"\n",
    "    elif \"Sales\" in file_path:\n",
    "        entity = \"Sales\"\n",
    "        domain = \"Commercial\"    \n",
    "    \n",
    "    integrated_path = f\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Integrated/Domain={domain}/Entity={entity}/schemaVersion=1/SubmissionYear={submission_year}/SubmissionMonth={submission_month}/SubmissionDay={submission_day}/SubmissionHour={submission_hour}/SubmissionMinute={submission_minute}/SubmissionSecond={submission_second}\"\n",
    "\n",
    "\n",
    "    if entity == \"Product\":\n",
    "        df.write.mode(\"overwrite\").parquet(integrated_path)\n",
    "    elif entity == \"Sales\":\n",
    "        df = df.withColumn(\"SalesOrderYear\", substring(\"SalesOrderDate\", 1, 4)).withColumn(\"SalesOrderMonth\", substring(\"SalesOrderDate\", 5, 2))\n",
    "        df.write.partitionBy(\"SalesOrderYear\", \"SalesOrderMonth\").mode(\"overwrite\").parquet(integrated_path)\n",
    "    else:\n",
    "        return print(\"Unknown parquet file.\")\n",
    "        \n",
    "transformer(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=Sales/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=20/SubmissionHour=12\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ac8bc8-fd8e-4ed8-8ace-32efb323abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------------------+\n",
      "|ProductCategoryID|ProductCategoryName|ProductSubCategory|\n",
      "+-----------------+-------------------+------------------+\n",
      "|                1|              Bikes|    Mountain Bikes|\n",
      "|                2|              Bikes|        Road Bikes|\n",
      "|                3|              Bikes|     Touring Bikes|\n",
      "|                4|         Components|        Handlebars|\n",
      "|                5|         Components|   Bottom Brackets|\n",
      "|                6|         Components|            Brakes|\n",
      "|                7|         Components|            Chains|\n",
      "|                8|         Components|         Cranksets|\n",
      "|                9|         Components|       Derailleurs|\n",
      "|               10|         Components|             Forks|\n",
      "|               11|         Components|          Headsets|\n",
      "|               12|         Components|   Mountain Frames|\n",
      "|               13|         Components|            Pedals|\n",
      "|               14|         Components|       Road Frames|\n",
      "|               15|         Components|           Saddles|\n",
      "|               16|         Components|    Touring Frames|\n",
      "|               17|         Components|            Wheels|\n",
      "|               18|           Clothing|        Bib-shorts|\n",
      "|               19|           Clothing|              Caps|\n",
      "|               20|           Clothing|            Gloves|\n",
      "+-----------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def read_latest_parquet(file_path):\n",
    "    latest_file = None\n",
    "    latest_timestamp = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            file_timestamp = os.path.getctime(root)\n",
    "\n",
    "            if file_timestamp > latest_timestamp:\n",
    "                latest_timestamp = file_timestamp\n",
    "                latest_file = root\n",
    "            \n",
    "    if latest_file:\n",
    "        df = spark.read.parquet(latest_file)\n",
    "        return df\n",
    "\n",
    "read_latest_parquet('C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=ProductCategory/').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4c6131d-0cee-4a87-a038-cccb937bbced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+----------------+----------------+---------+--------------+----------------+--------------------+----------------+--------------------+--------------------+-------------------+--------+---------+-----------------+---------+--------+\n",
      "|               _c0|         _c1|             _c2|             _c3|      _c4|           _c5|             _c6|                 _c7|             _c8|                 _c9|                _c10|               _c11|    _c12|     _c13|             _c14|     _c15|    date|\n",
      "+------------------+------------+----------------+----------------+---------+--------------+----------------+--------------------+----------------+--------------------+--------------------+-------------------+--------+---------+-----------------+---------+--------+\n",
      "|SalesOrderDetailID|SalesOrderID|SalesOrderNumber|       OrderDate|ProductID|SpecialOfferID|SpecialOfferName|SpecialOfferDisco...|SpecialOfferType|SpecialOfferCategory|SpecialOfferStart...|SpecialOfferEndDate|OrderQty|UnitPrice|UnitPriceDiscount|LineTotal|20220606|\n",
      "|              5717|       45266|         SO45266|01/01/2012 00:00|      777|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2| 2024.994|                0| 4049.988|20220606|\n",
      "|              5718|       45266|         SO45266|01/01/2012 00:00|      774|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       3| 2039.994|                0| 6119.982|20220606|\n",
      "|              5719|       45266|         SO45266|01/01/2012 00:00|      775|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2| 2024.994|                0| 4049.988|20220606|\n",
      "|              5720|       45266|         SO45266|01/01/2012 00:00|      743|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 714.7043|                0| 714.7043|20220606|\n",
      "|              5721|       45266|         SO45266|01/01/2012 00:00|      778|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2| 2024.994|                0| 4049.988|20220606|\n",
      "|              5722|       45266|         SO45266|01/01/2012 00:00|      772|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2| 2039.994|                0| 4079.988|20220606|\n",
      "|              5723|       45266|         SO45266|01/01/2012 00:00|      748|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2| 722.5949|                0|1445.1898|20220606|\n",
      "|              5724|       45267|         SO45267|01/01/2012 00:00|      762|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "|              5725|       45267|         SO45267|01/01/2012 00:00|      770|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "|              5726|       45267|         SO45267|01/01/2012 00:00|      758|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       3|  874.794|                0| 2624.382|20220606|\n",
      "|              5727|       45268|         SO45268|01/01/2012 00:00|      766|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "|              5728|       45268|         SO45268|01/01/2012 00:00|      760|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "|              5729|       45269|         SO45269|01/01/2012 00:00|      775|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 2024.994|                0| 2024.994|20220606|\n",
      "|              5730|       45270|         SO45270|01/01/2012 00:00|      766|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2| 419.4589|                0| 838.9178|20220606|\n",
      "|              5731|       45270|         SO45270|01/01/2012 00:00|      712|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       2|   5.1865|                0|   10.373|20220606|\n",
      "|              5732|       45270|         SO45270|01/01/2012 00:00|      763|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "|              5733|       45270|         SO45270|01/01/2012 00:00|      765|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "|              5734|       45270|         SO45270|01/01/2012 00:00|      749|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       3| 2146.962|                0| 6440.886|20220606|\n",
      "|              5735|       45270|         SO45270|01/01/2012 00:00|      767|             1|     No Discount|                   0|     No Discount|         No Discount|    01/05/2011 00:00|   30/11/2014 00:00|       1| 419.4589|                0| 419.4589|20220606|\n",
      "+------------------+------------+----------------+----------------+---------+--------------+----------------+--------------------+----------------+--------------------+--------------------+-------------------+--------+---------+-----------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Sourced/SystemA/Sales\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25ddd9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID        Level1           Level2\n",
      "0    1         bikes   mountain bikes\n",
      "1    2         bikes      road bikes \n",
      "2    3         Bikes    touring bikes\n",
      "3    4  components         handlebars\n",
      "4    5  components\\t  bottom brackets\n",
      "5    6  components\\t           brakes\n",
      "6    7  components\\t           chains\n",
      "7    8  components\\t        cranksets\n",
      "8    9    components      derailleurs\n",
      "9   10    components            forks\n",
      "10  11    components         headsets\n",
      "11  12    components  mountain frames\n",
      "12  13    components           pedals\n",
      "13  14    components      road frames\n",
      "14  15    components          saddles\n",
      "15  16    components   touring frames\n",
      "16  17    components           wheels\n",
      "17  18      clothing       Bib-shorts\n",
      "18  19      clothing             CAPS\n",
      "19  20      clothing           GLOVES\n",
      "20  21      clothing          JERSEYS\n",
      "21  22    clothing             SHORTS\n",
      "22  23    clothing              SOCKS\n",
      "23  24      clothing           helmet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    [\"1\", \"   bikes\", \"mountain bikes\"],\n",
    "    [\"2\", \"   bikes\", \"road bikes \"],\n",
    "    [\"3\", \"Bikes\", \"touring bikes\"],\n",
    "    [\"4\", \"components  \", \"handlebars\"],\n",
    "    [\"5\", \"components\t\", \"bottom brackets\"],\n",
    "    [\"6\", \"components\t\", \"brakes\"],\n",
    "    [\"7\", \"components\t\", \"chains\"],\n",
    "    [\"8\", \"components\t\", \"cranksets\"],\n",
    "    [\"9\", \"components\", \"derailleurs\"],\n",
    "    [\"10\", \"components\", \"forks\"],\n",
    "    [\"11\", \"components\", \"headsets\"],\n",
    "    [\"12\", \"components\", \"mountain frames\"],\n",
    "    [\"13\", \"components\", \"pedals\"],\n",
    "    [\"14\", \"components\", \"road frames\"],\n",
    "    [\"15\", \"components\", \"saddles\"],\n",
    "    [\"16\", \"components\", \"touring frames\"],\n",
    "    [\"17\", \"components\", \"wheels\"],\n",
    "    [\"18\", \"clothing\", \"Bib-shorts\"],\n",
    "    [\"19\", \"clothing\", \"CAPS\"],\n",
    "    [\"20\", \"clothing\", \"GLOVES\"],\n",
    "    [\"21\", \" clothing\", \"JERSEYS\"],\n",
    "    [\"22\", \"clothing  \", \"SHORTS\"],\n",
    "    [\"23\", \"clothing  \", \"SOCKS\"],\n",
    "    [\"24\", \"clothing\", \"helmet\"]\n",
    "]\n",
    "\n",
    "columns = [\"ID\", \"Level1\", \"Level2\"]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3a7c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def find_schema(schema):\n",
    "    if schema in locals():\n",
    "        return locals()[schema]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(find_schema(DimProduct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbd5ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|SalesOrderDate|\n",
      "+--------------+\n",
      "|      20140314|\n",
      "|      20140515|\n",
      "|      20140501|\n",
      "|      20140519|\n",
      "|      20130703|\n",
      "|      20130728|\n",
      "|      20140312|\n",
      "|      20130707|\n",
      "|      20140529|\n",
      "|      20130726|\n",
      "|      20140309|\n",
      "|      20140303|\n",
      "|      20140320|\n",
      "|      20140505|\n",
      "|      20130722|\n",
      "|      20140310|\n",
      "|      20140318|\n",
      "|      20140328|\n",
      "|      20140506|\n",
      "|      20140512|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+\n",
      "|  SalesOrderDate|\n",
      "+----------------+\n",
      "|17/06/2014 00:00|\n",
      "|17/05/2014 00:00|\n",
      "|14/06/2014 00:00|\n",
      "|19/04/2014 00:00|\n",
      "|24/03/2014 00:00|\n",
      "|24/06/2014 00:00|\n",
      "|29/01/2014 00:00|\n",
      "|20/02/2014 00:00|\n",
      "|03/04/2014 00:00|\n",
      "|02/06/2014 00:00|\n",
      "|25/05/2014 00:00|\n",
      "|06/06/2014 00:00|\n",
      "|18/02/2014 00:00|\n",
      "|09/05/2014 00:00|\n",
      "|23/02/2014 00:00|\n",
      "|10/04/2014 00:00|\n",
      "|17/03/2014 00:00|\n",
      "|03/03/2014 00:00|\n",
      "|21/04/2014 00:00|\n",
      "|07/06/2014 00:00|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Integrated/Domain=Commercial/Entity=Sales/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=31/SubmissionHour=10/SubmissionMinute=10\")\n",
    "df.select(\"SalesOrderDate\").distinct().show()\n",
    "\n",
    "df = spark.read.parquet(\"C:/Users/TobyBarker/Documents/Data_Engineering_Accelerator/src/databricks/sample_lake/Cleansed/DataFeed=Sales/schemaVersion=1/SubmissionYear=2023/SubmissionMonth=10/SubmissionDay=30\")\n",
    "df.select(\"SalesOrderDate\").distinct().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
